from typing import Type
from pydantic import BaseModel
import instructor
from cognee.infrastructure.llm.llm_interface import LLMInterface
from cognee.infrastructure.llm.config import get_llm_config
from openai import OpenAI
import httpx
import json
import re
import litellm
from typing import List, Dict, Any


class OllamaAPIAdapter(LLMInterface):
    """Adapter for a Generic API LLM provider using instructor with an OpenAI backend."""

    def __init__(self, endpoint: str, api_key: str, model: str, name: str, max_tokens: int):
        self.name = name
        self.model = model
        self.api_key = api_key
        self.endpoint = endpoint
        self.max_tokens = max_tokens

       

    async def acreate_structured_output(
        self, text_input: str, system_prompt: str, response_model: Type[BaseModel]
    ) -> BaseModel:
        print("!!!!!!!!!!!=====Call actreate_structured_output======!!!!!!!")

        # 让 `instructor` 适配 `litellm`
        class LitellmWrapper:
            def __init__(self, model: str, api_base: str, format:str):
                self.model = model
                self.api_base = api_base
                self.format = format

            def completion(self, messages: List[Dict[str, Any]], **kwargs):
                if self.format == "str":
                    return litellm.completion(
                        model=self.model,
                        api_base=self.api_base,
                        messages=messages 
                    )
                else:
                    return litellm.completion(
                        model=self.model,
                        api_base=self.api_base,
                        messages=messages,
                        format="json"
                    )


        if issubclass(response_model, BaseModel):
            schema = response_model.model_json_schema()
            print("======schema:", schema)
        else:
            schema = "str"
            print("======schema:", schema)

        # ✅ 创建 `litellm` 代理
        llm_adapter = LitellmWrapper(model="ollama/qwq", api_base="http://localhost:11434", format=schema)

       
    
        if schema=="str":
            # ✅ 让 `instructor` 适配 `litellm`
            self.aclient = instructor.from_litellm(llm_adapter.completion, mode=instructor.Mode.JSON)

            response = self.aclient.chat.completions.create(
                messages=[
                    {
                        "role": "user", 
                        "content": text_input
                    },
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    ],
                response_model=response_model
            )
        else:
            self.aclient = instructor.from_litellm(llm_adapter.completion, mode=instructor.Mode.JSON)
            response = self.aclient.chat.completions.create(
                messages=[
                    {
                        "role": "user", 
                        "content": f"Use the given format to extract information from the following input: {text_input}\n{system_prompt}"
                    }
                    ],
                response_model=response_model,  # 让 Ollama 直接返回符合 `Node` 结构的 JSON
            )
        

        print("!!!!!!!!!Debug=========:",response)
        return response
